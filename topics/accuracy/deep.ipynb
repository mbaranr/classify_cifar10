{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.plots import plot_losses, plot_confusion_matrix\n",
    "from utils.train import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ac94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the standard pre-computed values\n",
    "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar10_std  = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(\n",
    "    root=\"../../assets/cifar10\", \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=train_transforms\n",
    ")\n",
    "val_dataset = CIFAR10(\n",
    "    root=\"../../assets/cifar10\", \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, train_loader, val_loader, optimizer, loss_fn, device):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    pbar = tqdm(range(num_epochs), desc=\"Training\")\n",
    "\n",
    "    for _ in pbar:\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, optimizer, loss_fn, device\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc = evaluate(\n",
    "            model, val_loader, loss_fn, device\n",
    "        )\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"train_loss\": f\"{train_loss:.4f}\",\n",
    "            \"val_loss\": f\"{val_loss:.4f}\",\n",
    "            \"val_acc\": f\"{val_acc*100:.2f}%\"\n",
    "        })\n",
    "\n",
    "    print(f\"Final Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "    plot_losses([train_losses, val_losses], [\"Train Loss\", \"Validation Loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3cc58e",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "Adam (Adaptive Moment Estimation) is an optimizer that combines momentum and adaptive learning rates to make training faster and more robust. Works well with minimal tuning and converges quickly.\n",
    "\n",
    "Let $g_t$ be the gradient at time step $t$.\n",
    "\n",
    "### 1. First moment (momentum)\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "$$\n",
    "\n",
    "### 2. Second moment (RMS / variance)\n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "$$\n",
    "\n",
    "### 3. Bias correction\n",
    "\n",
    "Because both moments are initialized at zero, Adam applies bias correction:\n",
    "\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad\n",
    "\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "### 4. Parameter update\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\beta_1$ controls momentum (default: 0.9)\n",
    "- $\\beta_2$ controls variance smoothing (default: 0.999)\n",
    "- $\\epsilon$ is a small constant for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d03c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_helpers(model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999))\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    return optimizer, loss_fn, device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fde554",
   "metadata": {},
   "source": [
    "## MaxPool2d\n",
    "\n",
    "MaxPool2d downsamples a feature map by keeping only the strongest activation in each local window. It reduces spatial resolution while preserving the most salient features.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../../assets/img/accuracy/pooling_in_action.png\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc61b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    # feature extractor\n",
    "    nn.Conv2d(3, 32, kernel_size=3, padding=1),   # (bsz, 32, 32, 32)\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),                           # (bsz, 32, 16, 16)\n",
    "\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1),  # (bsz, 64, 16, 16)\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),                           # (bsz, 64, 8, 8)\n",
    "\n",
    "    # classifier\n",
    "    nn.Flatten(),                                 # (bsz, 64*8*8)\n",
    "    nn.Linear(64 * 8 * 8, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)                            # class logits\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1322b0f",
   "metadata": {},
   "source": [
    "Expect a final accuracy of **~76%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543554bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, loss_fn, device = get_helpers(model)\n",
    "train(model, 25, train_loader, val_loader, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33a544",
   "metadata": {},
   "source": [
    "## BatchNorm2d\n",
    "\n",
    "BatchNorm2d normalizes convolutional feature maps channel-wise to stabilize and speed up training.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../../assets/img/accuracy/normalization_techs.png\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20428ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    # feature extractor\n",
    "    nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "\n",
    "    # classifier\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64 * 8 * 8, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae34f1",
   "metadata": {},
   "source": [
    "Expect a final accuracy of **~76%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, loss_fn, device = get_helpers(model)\n",
    "train(model, 25, train_loader, val_loader, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Number of parameters: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb4ddc",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Dropout is a regularization technique that randomly disables neurons during training to reduce overfitting.\n",
    "\n",
    "p = probability that each activation is set to zero.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../../assets/img/accuracy/dropout.png\" width=\"400\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_conv_layers=3, base_channels=32, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        channels = base_channels\n",
    "        spatial_size = 32\n",
    "\n",
    "        for _ in range(num_conv_layers):\n",
    "            layers.append(ConvBlock(in_channels, channels))\n",
    "            in_channels = channels\n",
    "            channels *= 2\n",
    "            spatial_size //= 2\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels * spatial_size * spatial_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee0cf8",
   "metadata": {},
   "source": [
    "Expect a final accuracy of **~80%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00678e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN(num_conv_layers=3, base_channels=32, dropout_p=0.5)\n",
    "optimizer, loss_fn, device = get_helpers(model)\n",
    "train(model, 25, train_loader, val_loader, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0eabc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, val_loader, device, [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\"dog\", \"frog\", \"horse\", \"ship\", \"truck\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cifar-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
