{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a91167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from utils.train import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b70b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the standard pre-computed values\n",
    "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar10_std  = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "t = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "])\n",
    "\n",
    "ds = CIFAR10(\n",
    "    root=\"../../assets/cifar10\", \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=t\n",
    ")\n",
    "\n",
    "dl = DataLoader(ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ddafab",
   "metadata": {},
   "source": [
    "## Static Quantization\n",
    "\n",
    "Static quantization converts both weights and activations to low-precision integers ahead of inference using calibration data, enabling maximum compression and speedup on supported CPU backends.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../../assets/img/deployment/static_quantization.png\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec9d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class QuantizedCNN(nn.Module):\n",
    "    def __init__(self, base_channels=32, num_conv_layers=3, dropout_p=0.5):\n",
    "        super(QuantizedCNN, self).__init__()\n",
    "        \n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        channels = base_channels\n",
    "        spatial_size = 32\n",
    "\n",
    "        for _ in range(num_conv_layers):\n",
    "            layers.append(ConvBlock(in_channels, channels))\n",
    "            in_channels = channels\n",
    "            channels *= 2\n",
    "            spatial_size //= 2\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels * spatial_size * spatial_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        out = self.features(x)\n",
    "        out = self.classifier(out)\n",
    "        out = self.dequant(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06275812",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32 = QuantizedCNN(num_conv_layers=3, base_channels=32, dropout_p=0.5)\n",
    "model_fp32.load_state_dict(torch.load('../../assets/models/model_fp32.pth'), strict=False)\n",
    "model_fp32.eval()\n",
    "\n",
    "# fbgemm for Intel, AMD CPUS (Windows/Linux), qnnpack for ARM (mobile)\n",
    "torch.backends.quantized.engine = \"qnnpack\"\n",
    "model_fp32.qconfig = torch.quantization.get_default_qconfig(\"qnnpack\")\n",
    "\n",
    "torch.quantization.prepare(model_fp32, inplace=True)\n",
    "\n",
    "# claibration\n",
    "with torch.no_grad():\n",
    "    for images, _ in dl:\n",
    "        model_fp32(images)\n",
    "        break  # a few batches is enough\n",
    "\n",
    "model_int8 = torch.quantization.convert(model_fp32, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a225a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_int8.state_dict(), '../../assets/models/model_int8_static.pth')\n",
    "\n",
    "def get_file_size_mb(path):\n",
    "    return os.path.getsize(path) / (1024 ** 2)\n",
    "\n",
    "fp32_size = get_file_size_mb(\"../../assets/models/model_fp32.pth\")\n",
    "int8_size = get_file_size_mb(\"../../assets/models/model_int8_static.pth\")\n",
    "\n",
    "print(f\"FP32 model size: {fp32_size:.2f} MB\")\n",
    "print(f\"INT8 model size: {int8_size:.2f} MB\")\n",
    "print(f\"Compression ratio: {fp32_size / int8_size:.2f}×\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bdddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, fp32_acc = evaluate(model_fp32, dl, nn.CrossEntropyLoss(), \"cpu\")\n",
    "_, int8_acc = evaluate(model_int8, dl, nn.CrossEntropyLoss(), \"cpu\")\n",
    "\n",
    "print(f\"FP32 model accuracy: {fp32_acc:.2f}%\")\n",
    "print(f\"INT8 model accuracy: {int8_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87408dbb",
   "metadata": {},
   "source": [
    "## Dynamic Quantization\n",
    "\n",
    "Dynamic quantization quantizes weights ahead of time but quantizes activations on the fly during inference, requiring no calibration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0824c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_conv_layers=3, base_channels=32, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        channels = base_channels\n",
    "        spatial_size = 32\n",
    "\n",
    "        for _ in range(num_conv_layers):\n",
    "            layers.append(ConvBlock(in_channels, channels))\n",
    "            in_channels = channels\n",
    "            channels *= 2\n",
    "            spatial_size //= 2\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels * spatial_size * spatial_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b431e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN(num_conv_layers=3, base_channels=32, dropout_p=0.5)\n",
    "model_fp32.load_state_dict(torch.load(\"../../assets/models/model_fp32.pth\"))\n",
    "model_fp32.eval()\n",
    "\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_fp32,\n",
    "    {torch.nn.Linear},   # only quantize Linear layers\n",
    "    dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_int8.state_dict(), '../../assets/models/model_int8_dynamic.pth')\n",
    "\n",
    "def get_file_size_mb(path):\n",
    "    return os.path.getsize(path) / (1024 ** 2)\n",
    "\n",
    "fp32_size = get_file_size_mb(\"../../assets/models/model_fp32.pth\")\n",
    "int8_size = get_file_size_mb(\"../../assets/models/model_int8_dynamic.pth\")\n",
    "\n",
    "print(f\"FP32 model size: {fp32_size:.2f} MB\")\n",
    "print(f\"INT8 model size: {int8_size:.2f} MB\")\n",
    "print(f\"Compression ratio: {fp32_size / int8_size:.2f}×\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cifar-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
